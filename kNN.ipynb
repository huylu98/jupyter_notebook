{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pertvirt/jupyter_notebook/blob/master/kNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9NwR9FXZNm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar xzvf data.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JskUfGgX2jJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "1cf3ed11-e4bd-4115-974b-0c17edb586fc"
      },
      "source": [
        "# k-nearest neighbors\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "from math import sqrt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Load a file\n",
        "def load_data(filename):\n",
        "    dataset = list()\n",
        "    with open(filename, 'r') as file:\n",
        "        for row in file:\n",
        "            if not row:\n",
        "                continue\n",
        "            dataset.append(row.rstrip('\\n').split(','))\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "    for row in dataset:\n",
        "        row[column] = float(row[column].strip())\n",
        "\n",
        "\n",
        "# Find the min and max values for each column\n",
        "def dataset_minmax(dataset):\n",
        "    minmax = list()\n",
        "    for i in range(len(dataset[0])):\n",
        "        col_values = [row[i] for row in dataset]\n",
        "        value_min = min(col_values)\n",
        "        value_max = max(col_values)\n",
        "        minmax.append([value_min, value_max])\n",
        "    return minmax\n",
        "\n",
        "\n",
        "# Rescale dataset columns to the range 0-1\n",
        "def normalize_dataset(dataset, minmax):\n",
        "    for row in dataset:\n",
        "        for i in range(len(row)):\n",
        "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
        "\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "    dataset_split = list()\n",
        "    dataset_copy = list(dataset)\n",
        "    fold_size = int(len(dataset) / n_folds)\n",
        "    for _ in range(n_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "        dataset_split.append(fold)\n",
        "    return dataset_split\n",
        "\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "\n",
        "\n",
        "# version 2 - Evaluate an algorithm in general\n",
        "def evaluate_algorithm(train_set, test_set, algorithm, *args):\n",
        "    results = {}\n",
        "    predicted = algorithm(train_set, test_set, *args)\n",
        "    results['predicted'] = predicted\n",
        "    actual = [row[-1] for row in test_set]\n",
        "    results['actual'] = actual\n",
        "    accuracy = accuracy_metric(actual, predicted)\n",
        "    return (results, accuracy)\n",
        "\n",
        "\n",
        "# Calculate the Euclidean distance between two vectors\n",
        "def euclidean_distance(row1, row2):\n",
        "    distance = 0.0\n",
        "    for i in range(len(row1) - 1):\n",
        "        distance += (row1[i] - row2[i]) ** 2\n",
        "    return sqrt(distance)\n",
        "\n",
        "\n",
        "# Locate the most similar neighbors\n",
        "def get_neighbors(train, test_row, num_neighbors):\n",
        "    distances = list()\n",
        "    for train_row in train:\n",
        "        dist = euclidean_distance(test_row, train_row)\n",
        "        distances.append((train_row, dist))\n",
        "    distances.sort(key=lambda tup: tup[1])\n",
        "    neighbors = list()\n",
        "    for i in range(num_neighbors):\n",
        "        neighbors.append(distances[i][0])\n",
        "    return neighbors\n",
        "\n",
        "\n",
        "# Make a prediction with neighbors\n",
        "def predict_classification(train, test_row, num_neighbors):\n",
        "    neighbors = get_neighbors(train, test_row, num_neighbors)\n",
        "    output_values = [row[-1] for row in neighbors]\n",
        "    prediction = max(set(output_values), key=output_values.count)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "# kNN Algorithm\n",
        "def k_nearest_neighbors(train, test, num_neighbors):\n",
        "    predictions = list()\n",
        "    for row in test:\n",
        "        output = predict_classification(train, row, num_neighbors)\n",
        "        predictions.append(output)\n",
        "    return (predictions)\n",
        "\n",
        "\n",
        "# load training set and preprocess data\n",
        "seed(1)\n",
        "train_set = 'data/iris/iris.trn'\n",
        "loaded_train_set = load_data(train_set)\n",
        "\n",
        "for i in range(len(loaded_train_set[0]) - 1):\n",
        "    str_column_to_float(loaded_train_set, i)\n",
        "\n",
        "# load test set and preprocess data\n",
        "test_set = 'data/iris/iris.tst'\n",
        "loaded_test_set = load_data(test_set)\n",
        "\n",
        "for i in range(len(loaded_test_set[0]) - 1):\n",
        "    str_column_to_float(loaded_test_set, i)\n",
        "\n",
        "num_neighbors = 5\n",
        "(confusions, scores) = evaluate_algorithm(loaded_test_set, loaded_test_set, k_nearest_neighbors, num_neighbors)\n",
        "# print('Scores: %s' % scores)\n",
        "print('Accuracy: %.3f%%' % (scores))\n",
        "\n",
        "# Python script for confusion matrix creation \n",
        "actual = confusions['actual']\n",
        "predicted = confusions['predicted']\n",
        "results = confusion_matrix(actual, predicted)\n",
        "print('Confusion Matrix :')\n",
        "print(results)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 98.000%\n",
            "Confusion Matrix :\n",
            "[[17  0  0]\n",
            " [ 0 15  0]\n",
            " [ 0  1 17]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}